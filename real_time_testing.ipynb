{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecdb19b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bimsa\\.conda\\envs\\nlp\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bimsa\\.conda\\envs\\nlp\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bimsa\\.conda\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in C:\\Users\\bimsa/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Lambda, RandomHorizontalFlip\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo\n",
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")\n",
    "from IPython.display import HTML, display\n",
    "from base64 import b64encode\n",
    "import torchmetrics\n",
    "import time\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class OurModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(400, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define video transformation pipeline\n",
    "video_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key='video',\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(20),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "            RandomShortSideScale(min_size=248, max_size=256),\n",
    "            CenterCropVideo(224),\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "        ])\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Set up clip sampler\n",
    "clip_sampler = make_clip_sampler(\"uniform\", 2.0)\n",
    "\n",
    "# Load the trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OurModel()\n",
    "model.load_state_dict(torch.load(\"trained_model/efficient_x3d_xs_finetuned.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded and ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f1210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(video_path):\n",
    "    start_total = time.time()\n",
    "    \n",
    "    # Create dataset and loader for a single video\n",
    "    start_dataset = time.time()\n",
    "    sample = [(video_path, {\"label\": 0})]\n",
    "    dataset = LabeledVideoDataset(\n",
    "        labeled_video_paths=sample,\n",
    "        clip_sampler=clip_sampler,\n",
    "        transform=video_transform,\n",
    "        decode_audio=False,\n",
    "        decoder=\"pyav\"\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=1, num_workers=0)\n",
    "    end_dataset = time.time()\n",
    "    print(f\"Dataset and DataLoader creation time: {end_dataset - start_dataset:.3f} seconds\")\n",
    "    \n",
    "    # Load batch\n",
    "    start_batch = time.time()\n",
    "    batch = next(iter(loader))\n",
    "    video_tensor = batch['video'].to(device)\n",
    "    end_batch = time.time()\n",
    "    print(f\"Batch loading and tensor transfer to device time: {end_batch - start_batch:.3f} seconds\")\n",
    "    \n",
    "    # Model inference\n",
    "    start_infer = time.time()\n",
    "    with torch.no_grad():\n",
    "        logits = model(video_tensor)\n",
    "        prob = torch.sigmoid(logits)\n",
    "        pred = (prob > 0.5).long().item()\n",
    "    end_infer = time.time()\n",
    "    print(f\"Model inference time: {end_infer - start_infer:.3f} seconds\")\n",
    "    \n",
    "    end_total = time.time()\n",
    "    print(f\"Total prediction time: {end_total - start_total:.3f} seconds\")\n",
    "    \n",
    "    return pred, prob.item()\n",
    "\n",
    "\n",
    "\n",
    "def show_video(video_path, width=400):\n",
    "    mp4 = open(video_path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(f\"\"\"\n",
    "    <video width=\"{width}\" controls>\n",
    "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99287560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and DataLoader creation time: 0.001 seconds\n",
      "Batch loading and tensor transfer to device time: 4.110 seconds\n",
      "Model inference time: 22.924 seconds\n",
      "Total prediction time: 27.054 seconds\n",
      "Predicted class: NonViolence (Score: 0.0007)\n"
     ]
    }
   ],
   "source": [
    "video_path = \"unseen_testing_videos/t2.mp4\"\n",
    "pred, prob = predict_video(video_path)\n",
    "\n",
    "class_names = ['NonViolence', 'Violence']\n",
    "print(f\"Predicted class: {class_names[pred]} (Score: {prob:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6105c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "class_names = ['NonViolence', 'Violence']\n",
    "latest_prediction = (\"Analyzing...\", 0.0)\n",
    "lock = threading.Lock()\n",
    "\n",
    "def predict_clip_thread(video_clip_path):\n",
    "    global latest_prediction\n",
    "    pred, prob = predict_video(video_clip_path)\n",
    "    with lock:\n",
    "        latest_prediction = (class_names[pred], prob)\n",
    "\n",
    "def real_time_classify(video_path, output_path=\"output_prediction.mp4\", fps=30, chunk_duration=3):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    actual_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fps = actual_fps if fps is None else fps\n",
    "    chunk_size = int(fps * chunk_duration)\n",
    "\n",
    "    # Video writer for output\n",
    "    out_writer = cv2.VideoWriter(\n",
    "        output_path,\n",
    "        cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "        fps,\n",
    "        (width, height)\n",
    "    )\n",
    "\n",
    "    frame_buffer = []\n",
    "    frame_count = 0\n",
    "    clip_index = 0\n",
    "\n",
    "    print(\"Starting real-time classification and saving...\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_buffer.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "        # Every chunk_duration seconds, run prediction in background\n",
    "        if frame_count == chunk_size:\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "                temp_path = temp_file.name\n",
    "\n",
    "            temp_writer = cv2.VideoWriter(\n",
    "                temp_path,\n",
    "                cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                fps,\n",
    "                (frame.shape[1], frame.shape[0])\n",
    "            )\n",
    "            for f in frame_buffer:\n",
    "                temp_writer.write(f)\n",
    "            temp_writer.release()\n",
    "\n",
    "            threading.Thread(target=predict_clip_thread, args=(temp_path,), daemon=True).start()\n",
    "\n",
    "            frame_buffer = []\n",
    "            frame_count = 0\n",
    "            clip_index += 1\n",
    "\n",
    "        # Overlay prediction label on frame\n",
    "        label, score = latest_prediction\n",
    "        display_frame = frame.copy()\n",
    "        cv2.putText(display_frame, f\"{label} ({score:.2f})\", (10, 60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4)\n",
    "\n",
    "        # Show and save the processed frame\n",
    "        cv2.imshow(\"Real-time Classification\", display_frame)\n",
    "        out_writer.write(display_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Processing complete. Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5292e6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time classification and saving...\n",
      "Dataset and DataLoader creation time: 0.001 seconds\n",
      "Batch loading and tensor transfer to device time: 2.497 seconds\n",
      "Model inference time: 0.416 seconds\n",
      "Total prediction time: 2.919 seconds\n",
      "Dataset and DataLoader creation time: 0.001 seconds\n",
      "Batch loading and tensor transfer to device time: 1.757 seconds\n",
      "Model inference time: 0.318 seconds\n",
      "Total prediction time: 2.115 seconds\n",
      "Dataset and DataLoader creation time: 0.001 seconds\n",
      "Batch loading and tensor transfer to device time: 1.768 seconds\n",
      "Model inference time: 0.152 seconds\n",
      "Total prediction time: 1.926 seconds\n",
      "Dataset and DataLoader creation time: 0.001 seconds\n",
      "Batch loading and tensor transfer to device time: 1.700 seconds\n",
      "Model inference time: 0.395 seconds\n",
      "Total prediction time: 2.105 seconds\n",
      "Dataset and DataLoader creation time: 0.001 seconds\n",
      "Batch loading and tensor transfer to device time: 1.694 seconds\n",
      "Model inference time: 0.227 seconds\n",
      "Total prediction time: 1.948 seconds\n",
      "Dataset and DataLoader creation time: 0.000 seconds\n",
      "Batch loading and tensor transfer to device time: 1.679 seconds\n",
      "Model inference time: 0.208 seconds\n",
      "Total prediction time: 1.895 seconds\n",
      "Dataset and DataLoader creation time: 0.001 seconds\n",
      "Batch loading and tensor transfer to device time: 1.823 seconds\n",
      "Model inference time: 0.269 seconds\n",
      "Total prediction time: 2.099 seconds\n",
      "Dataset and DataLoader creation time: 0.001 seconds\n",
      "Batch loading and tensor transfer to device time: 1.794 seconds\n",
      "Model inference time: 0.307 seconds\n",
      "Total prediction time: 2.129 seconds\n",
      "Dataset and DataLoader creation time: 0.000 seconds\n",
      "Batch loading and tensor transfer to device time: 1.747 seconds\n",
      "Model inference time: 0.186 seconds\n",
      "Total prediction time: 1.964 seconds\n",
      "Dataset and DataLoader creation time: 0.000 seconds\n",
      "Batch loading and tensor transfer to device time: 1.689 seconds\n",
      "Model inference time: 0.320 seconds\n",
      "Total prediction time: 2.039 seconds\n",
      "Dataset and DataLoader creation time: 0.000 seconds\n",
      "Batch loading and tensor transfer to device time: 1.701 seconds\n",
      "Model inference time: 0.133 seconds\n",
      "Total prediction time: 1.874 seconds\n",
      "Dataset and DataLoader creation time: 0.000 seconds\n",
      "Batch loading and tensor transfer to device time: 1.747 seconds\n",
      "Model inference time: 0.296 seconds\n",
      "Total prediction time: 2.079 seconds\n",
      "Dataset and DataLoader creation time: 0.001 seconds\n",
      "Batch loading and tensor transfer to device time: 1.688 seconds\n",
      "Model inference time: 0.164 seconds\n",
      "Total prediction time: 1.887 seconds\n",
      "Dataset and DataLoader creation time: 0.001 seconds\n",
      "Batch loading and tensor transfer to device time: 1.661 seconds\n",
      "Model inference time: 0.320 seconds\n",
      "Total prediction time: 1.991 seconds\n",
      "Dataset and DataLoader creation time: 0.000 seconds\n",
      "Batch loading and tensor transfer to device time: 1.580 seconds\n",
      "Model inference time: 0.136 seconds\n",
      "Total prediction time: 1.736 seconds\n",
      "Dataset and DataLoader creation time: 0.000 seconds\n",
      "Batch loading and tensor transfer to device time: 1.945 seconds\n",
      "Model inference time: 0.381 seconds\n",
      "Total prediction time: 2.333 seconds\n",
      "Dataset and DataLoader creation time: 0.000 seconds\n",
      "Batch loading and tensor transfer to device time: 1.761 seconds\n",
      "Model inference time: 0.221 seconds\n",
      "Total prediction time: 1.985 seconds\n",
      "Dataset and DataLoader creation time: 0.001 seconds\n",
      "Batch loading and tensor transfer to device time: 2.056 seconds\n",
      "Model inference time: 0.244 seconds\n",
      "Total prediction time: 2.308 seconds\n",
      "Dataset and DataLoader creation time: 0.000 seconds\n",
      "Batch loading and tensor transfer to device time: 1.479 seconds\n",
      "Processing complete. Saved to: output_prediction.mp4\n",
      "Model inference time: 0.318 seconds\n",
      "Total prediction time: 1.805 seconds\n"
     ]
    }
   ],
   "source": [
    "video_path = \"unseen_testing_videos/footage.mp4\"\n",
    "real_time_classify(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
